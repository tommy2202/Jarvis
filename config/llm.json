{
  "schema_version": 1,
  "enabled": true,
  "mode": "external",
  "debug_log_prompts": false,
  "managed_kill_server_on_idle": false,
  "backends": {
    "llamacpp": {
      "type": "llamacpp_inprocess",
      "model_path": "",
      "n_ctx": 2048,
      "n_gpu_layers": 0,
      "n_threads": null
    },
    "ollama": {
      "type": "ollama_http",
      "base_url": "http://127.0.0.1:11434"
    }
  },
  "roles": {
    "chat": {
      "backend": "llamacpp",
      "model": "models/chat.gguf",
      "model_path": "models/chat.gguf",
      "base_url": "http://127.0.0.1:11434",
      "n_ctx": 2048,
      "n_gpu_layers": 0,
      "idle_unload_seconds": 45,
      "max_request_seconds": 60,
      "max_tokens": 512,
      "temperature": 0.7
    },
    "coder": {
      "backend": "llamacpp",
      "model": "models/coder.gguf",
      "model_path": "models/coder.gguf",
      "base_url": "http://127.0.0.1:11434",
      "n_ctx": 4096,
      "n_gpu_layers": 0,
      "idle_unload_seconds": 5,
      "max_request_seconds": 120,
      "max_tokens": 1024,
      "temperature": 0.2
    }
  },
  "watchdog": {
    "health_check_interval_seconds": 10,
    "restart_on_failure": true,
    "max_restart_attempts": 3
  },
  "lifecycle": {
    "idle_unload_seconds": 300,
    "request_timeout_seconds": 60,
    "max_concurrency_per_backend": 1
  },
  "security": {
    "never_log_prompts": true,
    "max_output_tokens_default": 512,
    "localhost_only_for_http_backends": true
  }
}
